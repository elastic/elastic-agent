inputs:
    - name: cel-logstash
      id: cel-logstash-${kubernetes.hints.container_id}
      type: cel
      use_output: default
      streams:
        - auth.basic.password: null
          auth.basic.user: null
          condition: ${kubernetes.hints.logstash.node_cel.enabled} == true and ${kubernetes.hints.logstash.enabled} == true
          config_version: "2"
          data_stream:
            dataset: logstash.node
            type: metrics
          interval: ${kubernetes.hints.logstash.node_cel.period|kubernetes.hints.logstash.period|'30s'}
          program: "get(state.url)\n.as(resp, bytes(resp.Body)\n.decode_json().as(body,\n  {\n    \"logstash\":{\n      \"elasticsearch\": has(body.pipelines) \n      ? {\n          \"cluster\":{\n            \"id\":body.pipelines.map(pipeline_name, pipeline_name != \".monitoring-logstash\", has(body.pipelines[pipeline_name].vertices)\n              ? body.pipelines[pipeline_name].vertices.map(vertex, has(vertex.cluster_uuid), vertex.cluster_uuid) \n              : []).flatten(),\n           }\n        }\n      : {},\n      \"node\":{\n        \"stats\":{\n           \"events\":body.events,\n           \"jvm\":{\n              \"uptime_in_millis\":body.jvm.uptime_in_millis,\n              \"mem\":[body.jvm['mem']].drop(\"pools\")[0],\n              \"threads\":body.jvm.threads\n            },\n           \"queue\":body.queue,\n           \"reloads\":body.reloads,\n           \"process\":body.process,\n           \"os\":{\n            \"cpu\":body.process.cpu,\n            \"cgroup\":has(body.os.group) ? body.os.cgroup : {},\n           },\n           \"logstash\":{\n             \"ephemeral_id\":body.ephemeral_id,\n             \"host\":body.host,\n             \"http_address\":body.http_address,\n             \"name\":body.name,\n             \"pipeline\":body.pipeline,\n             \"pipelines\":body.pipelines.map(pipeline, pipeline != '.monitoring-logstash', [pipeline]).flatten(),\n             \"snapshot\":body.snapshot,\n             \"status\":body.status,\n             \"uuid\":body.id,\n             \"version\":body.version,\n            }\n        }}\n      }})\n)\n.as(eve, {\n  \"events\":[eve]\n})"
          redact:
            fields: null
          resource.url: http://localhost:9600/_node/stats?graph=true&vertices=true
        - auth.basic.password: null
          auth.basic.user: null
          condition: ${kubernetes.hints.logstash.pipeline.enabled} == true and ${kubernetes.hints.logstash.enabled} == true
          config_version: "2"
          data_stream:
            dataset: logstash.pipeline
            type: metrics
          interval: ${kubernetes.hints.logstash.pipeline.period|kubernetes.hints.logstash.period|'30s'}
          program: |
            get(state.url).as(resp, bytes(resp.Body).decode_json().as(body,
              body.pipelines.map(pipeline_name, pipeline_name != ".monitoring-logstash", {
                "name": pipeline_name,
                "elasticsearch.cluster.id": has(body.pipelines[pipeline_name].vertices) ?
                  body.pipelines[pipeline_name].vertices.map(vertex, has(vertex.cluster_uuid), vertex.cluster_uuid)
                :
                  [],
                "host":{
                  "name":body.name,
                  "address":body.http_address,
                },
                "total":{
                  "flow":body.pipelines[pipeline_name].flow,
                  "time":{
                    "queue_push_duration": {
                      "ms": has(body.pipelines[pipeline_name].events.queue_push_duration_in_millis) ?
                        body.pipelines[pipeline_name].events.queue_push_duration_in_millis
                      :
                        [],
                    },
                    "duration":{
                      "ms": has(body.pipelines[pipeline_name].events.duration_in_millis) ?
                        body.pipelines[pipeline_name].events.duration_in_millis
                      :
                        [],
                    },
                  },
                  "reloads":{
                    "successes":body.pipelines[pipeline_name].reloads.successes,
                    "failures":body.pipelines[pipeline_name].reloads.failures
                  },
                  "events":{
                    "out": has(body.pipelines[pipeline_name].events.out) ?
                      body.pipelines[pipeline_name].events.out
                    :
                      [],
                    "in": has(body.pipelines[pipeline_name].events.out) ? // This deliberately uses 'out' as `has` does not accept `in`
                      body.pipelines[pipeline_name].events['in']
                    :
                      [],
                    "filtered": has(body.pipelines[pipeline_name].events.filtered) ?
                      body.pipelines[pipeline_name].events.filtered
                    :
                      [],
                  },
                  "queues":{
                    "type": has(body.pipelines[pipeline_name].queue.type) ?
                      body.pipelines[pipeline_name].queue.type
                    :
                      [],
                    "events": has(body.pipelines[pipeline_name].queue.events_count) ?
                      body.pipelines[pipeline_name].queue.events_count
                    :
                      [],
                    "current_size":{
                      "bytes": has(body.pipelines[pipeline_name].queue.queue_size_in_bytes) ?
                        body.pipelines[pipeline_name].queue.queue_size_in_bytes
                      :
                        [],
                    },
                    "max_size":{
                      "bytes": has(body.pipelines[pipeline_name].queue.max_queue_size_in_bytes) ?
                        body.pipelines[pipeline_name].queue.max_queue_size_in_bytes
                      :
                        [],
                    }
                  }
                }
              }))).as(pipelines, {
                "events": pipelines.map(pipeline, {
                  "logstash": {"pipeline":pipeline}
                })
              })
          redact:
            fields: null
          resource.url: http://localhost:9600/_node/stats?graph=true&vertices=true
        - auth.basic.password: null
          auth.basic.user: null
          condition: ${kubernetes.hints.logstash.plugins.enabled} == true and ${kubernetes.hints.logstash.enabled} == true
          config_version: "2"
          data_stream:
            dataset: logstash.plugins
            type: metrics
          interval: ${kubernetes.hints.logstash.plugins.period|kubernetes.hints.logstash.period|'1m'}
          program: |
            get(state.url + "/stats?graph=true&vertices=true").as(resp, bytes(resp.Body).decode_json().as(body,
              body.pipelines.map(pipeline_name, pipeline_name != ".monitoring-logstash", body.pipelines[pipeline_name].with({
                "name":pipeline_name,
                "pipeline_source_map":
                  get(state.url + "/pipelines/" + pipeline_name + "?graph=true&vertices=true").as(resp,
                    bytes(resp.Body).decode_json().as(pipes,
                      has(pipes.pipeline) ?
                        pipes.pipelines.map(pipeline_name,
                          has(pipes.pipelines) && has(pipes.pipelines[pipeline_name].graph) && pipes.pipelines != null && pipes.pipelines[pipeline_name].graph.graph.vertices != null,
                          pipes.pipelines[pipeline_name].graph.graph.vertices.map(vertex, vertex.type == "plugin", {
                            "plugin_id": vertex.id,
                            "source": vertex.meta.source,
                          })
                        ).drop("graph").flatten()
                      :
                        []
                  )
                ),
                "es_cluster_id": has(body.pipelines[pipeline_name].vertices) ?
                  body.pipelines[pipeline_name].vertices.map(vertex, has(vertex.cluster_uuid), vertex.cluster_uuid)
                :
                  [],
                "es_cluster_id_map": has(body.pipelines[pipeline_name].vertices) ?
                  body.pipelines[pipeline_name].vertices.map(vertex, has(vertex.cluster_uuid), {
                    "plugin_id": vertex.id,
                    "cluster_id": vertex.cluster_uuid,
                  })
                :
                  [],
                "counter_map": has(body.pipelines[pipeline_name].vertices) ?
                  body.pipelines[pipeline_name].vertices.map(vertex, has(vertex.long_counters), vertex.long_counters.map(counter, {
                    "plugin_id": vertex.id,
                    "name": counter.name,
                    "value": counter.value
                  }))
                :
                  [],
                "outputs": body.pipelines[pipeline_name].plugins.outputs,
                "inputs": body.pipelines[pipeline_name].plugins.inputs,
                "filters": body.pipelines[pipeline_name].plugins.filters,
                "codecs": body.pipelines[pipeline_name].plugins.codecs,
                "host":{
                  "name": body.name,
                  "address": body.http_address,
                }
              })))).as(events, events.map(event, {
                "inputs": event.inputs.map(input, has(event.hash), {
                  "name": event.name,
                  "id": event.hash,
                  "host": event.host,
                  "elasticsearch.cluster.id": event.es_cluster_id,
                  "plugin": {
                    "type": "input",
                    "input": {
                      "source":event.pipeline_source_map.map(tuple, (tuple.plugin_id == input.id), tuple.source).flatten().as(source, (source.size() != 0) ? source[0] : ""),
                      "elasticsearch.cluster.id": event.es_cluster_id_map.map(tuple, tuple.plugin_id == input.id, tuple.cluster_id),
                      "metrics": {
                        input.name: event.counter_map.flatten().filter(tuple, tuple.plugin_id == input.id).as(counter_map, zip(
                          counter_map.map(tuple, tuple.name),
                          counter_map.map(tuple, tuple.value)
                        ))
                       },
                      "name": input.name,
                      "id": input.id,
                      "flow": has(input.flow) ?
                        input.flow
                      :
                        {},
                      "events": {
                        "out": input.events.out,
                      },
                      "time": {
                        "queue_push_duration": {
                          "ms": input.events.queue_push_duration_in_millis
                        }
                      }
                    }
                  }
                }.drop_empty()),
                "codecs": event.codecs.map(codec, has(event.hash), {
                  "name": event.name,
                  "id": event.hash,
                  "host": event.host,
                  "elasticsearch.cluster.id": event.es_cluster_id,
                  "plugin": {
                    "type": "codec",
                    "codec": {
                    "id":codec.id,
                    "name":codec.name,
                      "flow": has(codec.flow) ? codec.flow : {},
                      "decode":{
                        "duration":{
                          "ms":codec.decode.duration_in_millis
                        },
                        "in":codec.decode.writes_in,
                        "out":codec.decode.out,
                      },
                      "encode":{
                        "in":codec.encode.writes_in,
                        "duration":{
                          "ms":codec.encode.duration_in_millis
                        }
                      }
                    }
                  }
                }.drop_empty()),
                "filters": event.filters.map(filter, has(event.hash), {
                  "name": event.name,
                  "id": event.hash,
                  "host": event.host,
                  "elasticsearch.cluster.id": event.es_cluster_id,
                  "plugin": {
                    "type": "filter",
                    "filter": {
                      "source":event.pipeline_source_map.map(tuple, (tuple.plugin_id == filter.id), tuple.source).flatten().as(source, (source.size() != 0) ? source[0] : ""),
                      "id": filter.id,
                      "name": filter.name,
                      "elasticsearch.cluster.id": event.es_cluster_id_map.map(tuple, tuple.plugin_id == filter.id, tuple.cluster_id),
                      "metrics": {
                        filter.name: event.counter_map.flatten().filter(tuple, tuple.plugin_id == filter.id).as(counter_map, zip(
                          counter_map.map(tuple, tuple.name),
                          counter_map.map(tuple, tuple.value)
                        ))
                      },
                      "flow": has(filter.flow) ?
                        filter.flow
                      :
                        {},
                      "events": {
                        "in": filter.events['in'],
                        "out": filter.events.out,
                      },
                      "time": {
                        "duration": {
                          "ms": filter.events.duration_in_millis
                        }
                      }
                    }
                  }
                }.drop_empty()),
                "outputs": event.outputs.map(output, has(event.hash), {
                  "name": event.name,
                  "id": event.hash,
                  "host": event.host,
                  "elasticsearch.cluster.id": event.es_cluster_id,
                  "plugin": {
                    "type": "output",
                    "output": {
                      "id": output.id,
                      "name": output.name,
                      "source":event.pipeline_source_map.map(tuple, (tuple.plugin_id == output.id), tuple.source).flatten().as(source, (source.size() != 0) ? source[0] : ""),
                      "elasticsearch.cluster.id": event.es_cluster_id_map.map(tuple, tuple.plugin_id == output.id, tuple.cluster_id),
                      "metrics": {
                        output.name: event.counter_map.flatten().filter(tuple, tuple.plugin_id == output.id).as(counter_map, zip(
                          counter_map.map(tuple, tuple.name),
                          counter_map.map(tuple, tuple.value)
                        ))
                      },
                      "flow": has(output.flow) ?
                        output.flow
                      :
                        {},
                      "events":{
                        "in":output.events['in'],
                        "out":output.events.out,
                      },
                      "time":{
                        "duration":{
                          "ms":output.events.duration_in_millis
                        }
                      }
                    }
                  }
                }.drop_empty())
              }).collate(["filters", "outputs", "inputs", "codecs"])).as(plugins, {
                "events": plugins.map(plugin, {
                  "logstash":{"pipeline":plugin}
                })
              })
          redact:
            fields: null
          resource.url: http://localhost:9600/_node
      data_stream.namespace: default
    - name: filestream-logstash
      id: filestream-logstash-${kubernetes.hints.container_id}
      type: filestream
      use_output: default
      streams:
        - condition: ${kubernetes.hints.logstash.log.enabled} == true or ${kubernetes.hints.logstash.enabled} == true
          data_stream:
            dataset: logstash.log
            type: logs
          exclude_files:
            - .gz$
          file_identity:
            fingerprint: null
          id: filestream-logstash-logstash-log-${kubernetes.hints.container_id}
          multiline:
            match: after
            negate: true
            pattern: ^((\[[0-9]{4}-[0-9]{2}-[0-9]{2}[^\]]+\])|({.+}))
          parsers:
            - container:
                format: auto
                stream: ${kubernetes.hints.logstash.log.stream|'all'}
          paths:
            - /var/log/containers/*${kubernetes.hints.container_id}.log
          processors:
            - add_locale.when.not.regexp.message: ^{
            - add_fields:
                fields:
                    ecs.version: 1.10.0
                target: ""
          prospector:
            scanner:
                fingerprint:
                    enabled: true
                symlinks: true
        - condition: ${kubernetes.hints.logstash.slowlog.enabled} == true or ${kubernetes.hints.logstash.enabled} == true
          data_stream:
            dataset: logstash.slowlog
            type: logs
          exclude_files:
            - .gz$
          file_identity:
            fingerprint: null
          id: filestream-logstash-logstash-slowlog-${kubernetes.hints.container_id}
          parsers:
            - container:
                format: auto
                stream: ${kubernetes.hints.logstash.slowlog.stream|'all'}
          paths:
            - /var/log/containers/*${kubernetes.hints.container_id}.log
          processors:
            - add_locale.when.not.regexp.message: ^{
            - add_fields:
                fields:
                    ecs.version: 1.10.0
                target: ""
          prospector:
            scanner:
                fingerprint:
                    enabled: true
                symlinks: true
      data_stream.namespace: default
    - name: logstash/metrics-logstash
      id: logstash/metrics-logstash-${kubernetes.hints.container_id}
      type: logstash/metrics
      use_output: default
      streams:
        - condition: ${kubernetes.hints.logstash.node.enabled} == true or ${kubernetes.hints.logstash.enabled} == true
          data_stream:
            dataset: logstash.stack_monitoring.node
            type: metrics
          hosts:
            - ${kubernetes.hints.logstash.node.host|kubernetes.hints.logstash.host|'http://localhost:9600'}
          metricsets:
            - node
          password: ${kubernetes.hints.logstash.node.password|kubernetes.hints.logstash.password|''}
          period: ${kubernetes.hints.logstash.node.period|kubernetes.hints.logstash.period|'10s'}
          username: ${kubernetes.hints.logstash.node.username|kubernetes.hints.logstash.username|''}
        - condition: ${kubernetes.hints.logstash.node_stats.enabled} == true or ${kubernetes.hints.logstash.enabled} == true
          data_stream:
            dataset: logstash.stack_monitoring.node_stats
            type: metrics
          hosts:
            - ${kubernetes.hints.logstash.node_stats.host|kubernetes.hints.logstash.host|'http://localhost:9600'}
          metricsets:
            - node_stats
          password: ${kubernetes.hints.logstash.node_stats.password|kubernetes.hints.logstash.password|''}
          period: ${kubernetes.hints.logstash.node_stats.period|kubernetes.hints.logstash.period|'10s'}
          username: ${kubernetes.hints.logstash.node_stats.username|kubernetes.hints.logstash.username|''}
      data_stream.namespace: default
